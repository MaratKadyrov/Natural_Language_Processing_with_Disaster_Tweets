# Natural_Language_Processing_with_Disaster_Tweets

Competition link - [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started/overview)

![plot](https://github.com/MaratKadyrov/Natural_Language_Processing_with_Disaster_Tweets/blob/main/result.jpg)

What has been done:
- Data preprocessing (RE, spacy.lemmatizater).

Models:
- Two nn.Linear model (__Score: 0.79589__);
- Embedding+LSTM model (__Score: 0.76034__);
- Glove embedding model (__Score: 0.76800__);
- Fine-tunning BERT model (__Score: 0.81366__);
- Fine tunning RoBERTa model (__Score: 0.81612__) - the best model
